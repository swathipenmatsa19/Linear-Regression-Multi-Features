{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, you need to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Problem Statement ##\n",
    "\n",
    "You are given a dataset containing:\n",
    "    - a training set for a linear function\n",
    "    - a test set for testing the learned hypothesis function\n",
    "    \n",
    "You will build a simple linear regression algorithm that can correctly identify the parameters of w0 and w1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for W and initializes w_0 to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the W vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    W -- initialized vector of shape (dim, 1)\n",
    "    w_0 -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    # Hint: you can use np.zeros to initialize W\n",
    "    W = np.zeros([dim,1], dtype=float)\n",
    "    w_0 = 0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(W.shape == (dim, 1))\n",
    "    assert(isinstance(w_0, float) or isinstance(w_0, int))\n",
    "    return W, w_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Gradient Descent ##\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $h_{W}(X) = W^T * X + w_{0}\\tag{1}$\n",
    "- You calculate the cost function:  $$E(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2\\tag{2}$$. \n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial w_{j}} = \\frac{1}{m} \\sum_{i=1}^m (( h_{W}(x^{(i)}) -y^{(i)}) * x_{j}^{(i)})\\tag{3}$$\n",
    "$$ \\frac{\\partial E}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (h_{W}(x^{(i)}) -y^{(i)})\\tag{4}$$\n",
    "\n",
    "The weights will be updated:\n",
    "$$ w_{j} = w_{j} - {\\alpha} * \\frac{\\partial E}{\\partial w_{j}}\\tag{5}$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Algorithm\n",
    "\n",
    "def gradient_descent(W, w_0, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes W by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    W -- the weight vector\n",
    "    w_0 -- bias, a scalar\n",
    "    X -- data of the single feature\n",
    "    Y -- true \"label\" vector \n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You need to finish the following steps:\n",
    "        1) Calculate the cost and the gradient for the current parameters. \n",
    "        2) Update the parameters using gradient descent rule for w_0 and w_1.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Get the number of training examples\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        # Calculate the heuristic function: h(x) = W.T * X + w_0\n",
    "        Y_hat = np.dot(W.T,X)+ w_0\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "        ### START CODE HERE ### \n",
    "        # Calculate cost, dw, and dw_0\n",
    "        \n",
    "        cost = np.sum(np.square(np.subtract(Y_hat,Y)))\n",
    "        dw = np.dot(X, np.subtract(Y_hat,Y).T)\n",
    "        dw_0 = np.sum(np.subtract(Y_hat,Y).T)\n",
    "        \n",
    "        cost /= 2 * m\n",
    "        dw = np.divide(dw, m)\n",
    "        dw_0 = np.divide(dw_0, m)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        # Update W and w_0\n",
    "        \n",
    "        W = np.subtract(W, np.multiply(learning_rate, dw))\n",
    "        w_0 = np.subtract(w_0, np.multiply(learning_rate, dw_0))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "        if((i % 100) == 0):\n",
    "            costs.append(cost)\n",
    "            \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        \n",
    "        \n",
    "    params = {\n",
    "        \"W\": W,\n",
    "        \"w_0\": w_0\n",
    "    }\n",
    "\n",
    "    grads = {\n",
    "        \"dw\": dw,\n",
    "        \"dw_0\": dw_0\n",
    "    }\n",
    "        \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions ###\n",
    "The predicted output is calculated as $h_{W}(X) = W^T * X + w_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(W, w_0, X):\n",
    "    '''\n",
    "    Predict the real values using learned parameters (W, w_0)\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size \n",
    "    w_0 -- bias, a scalar\n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    W = W.reshape(X.shape[0], 1)\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    Y_prediction = np.dot(W.T,X)+ w_0\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling ###\n",
    "Here you normalize features using:\n",
    "$ \\frac{x_{i} - mean}{\\sigma}$, where $\\sigma$ is the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mtx):\n",
    "    '''\n",
    "    mtx: the matrix that needs to be normalized. Note that each column represents a training example. \n",
    "         The number of columns is the the number of training examples\n",
    "    '''\n",
    "    # Calculate mean for each feature\n",
    "    # Pay attention to the value of axis = ?\n",
    "    ### START CODE HERE ###\n",
    "    mean = np.mean(mtx)\n",
    "    std = np.std(mtx)\n",
    "    mtx = (mtx-mean) /std\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.1, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the linear regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array \n",
    "    Y_train -- training labels represented by a numpy array (vector) \n",
    "    X_test -- test set represented by a numpy array\n",
    "    Y_test -- test labels represented by a numpy array (vector)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    dim = X_train.shape[0]\n",
    "    W, w_0 = initialize_with_zeros(dim)\n",
    "    \n",
    "    #X_train = normalize(X_train)\n",
    "    #X_test = normalize(X_test)\n",
    "    \n",
    "    \n",
    "    # Gradient descent \n",
    "    ### START CODE HERE ###\n",
    "    parameters, grads, costs = gradient_descent( W, w_0, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve parameters w and w_0 from dictionary \"parameters\"\n",
    "    W = parameters[\"W\"]\n",
    "    w_0 = parameters[\"w_0\"]\n",
    "    \n",
    "    # Predict test/train set examples (â‰ˆ 2 lines of code)\n",
    "    ### START CODE HERE ###\n",
    "    Y_prediction_train = predict(W, w_0, X_train)\n",
    "    Y_prediction_test =  predict(W, w_0, X_test)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    print(\"w is \", W)\n",
    "    print(\"w_0 is \", w_0)\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"W\" : W, \n",
    "         \"w_0\" : w_0,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Start the Learning Process ###\n",
    "You can change num_iterations and learning_rate to see the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 65591548106.457443\n",
      "Cost after iteration 100: nan\n",
      "Cost after iteration 200: nan\n",
      "Cost after iteration 300: nan\n",
      "Cost after iteration 400: nan\n",
      "Cost after iteration 500: nan\n",
      "Cost after iteration 600: nan\n",
      "Cost after iteration 700: nan\n",
      "Cost after iteration 800: nan\n",
      "Cost after iteration 900: nan\n",
      "Cost after iteration 1000: nan\n",
      "Cost after iteration 1100: nan\n",
      "Cost after iteration 1200: nan\n",
      "Cost after iteration 1300: nan\n",
      "Cost after iteration 1400: nan\n",
      "Cost after iteration 1500: nan\n",
      "Cost after iteration 1600: nan\n",
      "Cost after iteration 1700: nan\n",
      "train accuracy: nan %\n",
      "test accuracy: nan %\n",
      "w is  [[nan]\n",
      " [nan]]\n",
      "w_0 is  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:44: RuntimeWarning: overflow encountered in square\n",
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:58: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('prj2house.csv', header=None)\n",
    "X_train = df[[0, 1]].values.T\n",
    "Y_train = df[2].values.reshape(-1, 1).T\n",
    "\n",
    "\n",
    "df_test = pd.read_csv('prj2house_test.csv', header=None)\n",
    "X_test = df_test[[0, 1]].values.T\n",
    "Y_test = df_test[2].values.reshape(-1, 1).T\n",
    "\n",
    "\n",
    "d = model(X_train, Y_train, X_test, Y_test, num_iterations = 1800, learning_rate = 0.01, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the learning curve ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
