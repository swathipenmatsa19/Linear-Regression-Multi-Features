{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, you need to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Problem Statement ##\n",
    "\n",
    "You are given a dataset containing:\n",
    "    - a training set for a linear function\n",
    "    - a test set for testing the learned hypothesis function\n",
    "    \n",
    "You will build a simple linear regression algorithm that can correctly identify the parameters of w0 and w1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for W and initializes w_0 to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the W vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    W -- initialized vector of shape (dim, 1)\n",
    "    w_0 -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    # Hint: you can use np.zeros to initialize W\n",
    "    W = np.zeros([dim,1], dtype=float)\n",
    "    w_0 = 0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(W.shape == (dim, 1))\n",
    "    assert(isinstance(w_0, float) or isinstance(w_0, int))\n",
    "    return W, w_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Gradient Descent ##\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $h_{W}(X) = W^T * X + w_{0}\\tag{1}$\n",
    "- You calculate the cost function:  $$E(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2\\tag{2}$$. \n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial w_{j}} = \\frac{1}{m} \\sum_{i=1}^m (( h_{W}(x^{(i)}) -y^{(i)}) * x_{j}^{(i)})\\tag{3}$$\n",
    "$$ \\frac{\\partial E}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (h_{W}(x^{(i)}) -y^{(i)})\\tag{4}$$\n",
    "\n",
    "The weights will be updated:\n",
    "$$ w_{j} = w_{j} - {\\alpha} * \\frac{\\partial E}{\\partial w_{j}}\\tag{5}$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Algorithm\n",
    "\n",
    "def gradient_descent(W, w_0, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes W by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    W -- the weight vector\n",
    "    w_0 -- bias, a scalar\n",
    "    X -- data of the single feature\n",
    "    Y -- true \"label\" vector \n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You need to finish the following steps:\n",
    "        1) Calculate the cost and the gradient for the current parameters. \n",
    "        2) Update the parameters using gradient descent rule for w_0 and w_1.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Get the number of training examples\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        # Calculate the heuristic function: h(x) = W.T * X + w_0\n",
    "        Y_hat = np.dot(W.T,X) + w_0\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "        ### START CODE HERE ### \n",
    "        # Calculate cost, dw, and dw_0\n",
    "       \n",
    "       \n",
    "        cost = np.sum(np.square(np.subtract(Y_hat,Y)))\n",
    "        dw = np.dot(X, np.subtract(Y_hat,Y).T)\n",
    "        dw_0 = np.sum(np.subtract(Y_hat,Y).T)\n",
    "        \n",
    "        cost /= 2 * m\n",
    "        dw = np.divide(dw, m)\n",
    "        dw_0 = np.divide(dw_0, m)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        # Update W and w_0\n",
    "        \n",
    "        W = np.subtract(W, np.multiply(learning_rate, dw))\n",
    "        w_0 = np.subtract(w_0, np.multiply(learning_rate, dw_0))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "        if((i % 100) == 0):\n",
    "            costs.append(cost)\n",
    "            \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        \n",
    "        \n",
    "    params = {\n",
    "        \"W\": W,\n",
    "        \"w_0\": w_0\n",
    "    }\n",
    "\n",
    "    grads = {\n",
    "        \"dw\": dw,\n",
    "        \"dw_0\": dw_0\n",
    "    }\n",
    "        \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions ###\n",
    "The predicted output is calculated as $h_{W}(X) = W^T * X + w_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(W, w_0, X):\n",
    "    '''\n",
    "    Predict the real values using learned parameters (W, w_0)\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size \n",
    "    w_0 -- bias, a scalar\n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    W = W.reshape(X.shape[0], 1)\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    Y_prediction = np.dot(W.T,X)+ w_0\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling ###\n",
    "Here you normalize features using:\n",
    "$ \\frac{x_{i} - mean}{\\sigma}$, where $\\sigma$ is the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mtx):\n",
    "    '''\n",
    "    mtx: the matrix that needs to be normalized. Note that each column represents a training example. \n",
    "         The number of columns is the the number of training examples\n",
    "    '''\n",
    "    # Calculate mean for each feature\n",
    "    # Pay attention to the value of axis = ?\n",
    "    ### START CODE HERE ###\n",
    "    mean = np.mean(mtx)\n",
    "    std = np.std(mtx)\n",
    "    mtx = (mtx-mean) /std\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.1, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the linear regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array \n",
    "    Y_train -- training labels represented by a numpy array (vector) \n",
    "    X_test -- test set represented by a numpy array\n",
    "    Y_test -- test labels represented by a numpy array (vector)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    dim = X_train.shape[0]\n",
    "    W, w_0 = initialize_with_zeros(dim)\n",
    "    \n",
    "    #X_train = normalize(X_train)\n",
    "    #X_test = normalize(X_test)\n",
    "    \n",
    "    \n",
    "    # Gradient descent \n",
    "    ### START CODE HERE ###\n",
    "    parameters, grads, costs = gradient_descent( W, w_0, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve parameters w and w_0 from dictionary \"parameters\"\n",
    "    W = parameters[\"W\"]\n",
    "    w_0 = parameters[\"w_0\"]\n",
    "    \n",
    "    # Predict test/train set examples (â‰ˆ 2 lines of code)\n",
    "    ### START CODE HERE ###\n",
    "    Y_prediction_train = predict(W, w_0, X_train)\n",
    "    Y_prediction_test =  predict(W, w_0, X_test)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    print(\"w is \", W)\n",
    "    print(\"w_0 is \", w_0)\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"W\" : W, \n",
    "         \"w_0\" : w_0,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Start the Learning Process ###\n",
    "You can change num_iterations and learning_rate to see the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 370.221965\n",
      "Cost after iteration 100: 1.286782\n",
      "Cost after iteration 200: 0.986669\n",
      "Cost after iteration 300: 0.761651\n",
      "Cost after iteration 400: 0.590167\n",
      "Cost after iteration 500: 0.459478\n",
      "Cost after iteration 600: 0.359877\n",
      "Cost after iteration 700: 0.283970\n",
      "Cost after iteration 800: 0.226121\n",
      "Cost after iteration 900: 0.182033\n",
      "Cost after iteration 1000: 0.148433\n",
      "Cost after iteration 1100: 0.122826\n",
      "Cost after iteration 1200: 0.103311\n",
      "Cost after iteration 1300: 0.088438\n",
      "Cost after iteration 1400: 0.077103\n",
      "Cost after iteration 1500: 0.068465\n",
      "Cost after iteration 1600: 0.061881\n",
      "Cost after iteration 1700: 0.056864\n",
      "train accuracy: 72.97682780823668 %\n",
      "test accuracy: 66.42009690227272 %\n",
      "w is  [[3.03722355]\n",
      " [2.07097267]]\n",
      "w_0 is  5.18746906121653\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('prj2data1.csv', header=None)\n",
    "X_train = df[[0, 1]].values.T\n",
    "Y_train = df[2].values.reshape(-1, 1).T\n",
    "\n",
    "\n",
    "df_test = pd.read_csv('prj2data1_test.csv', header=None)\n",
    "X_test = df_test[[0, 1]].values.T\n",
    "Y_test = df_test[2].values.reshape(-1, 1).T\n",
    "\n",
    "\n",
    "d = model(X_train, Y_train, X_test, Y_test, num_iterations = 1800, learning_rate = 0.01, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the learning curve ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de5wcdZnv8c83gQzIhOsMMSbBgEbxsnLZiLioi8BR4HgMetCF40pEdqMunPV2jqLuWXGV8/KGHvGCBrmEFRUE0SziJSLKoofLwIZAuEjAIDEhmZBwE0SSPPtH/TpT06nu6RmmujtT3/fr1a+u/tWvqp+uTPrp+lXVU4oIzMzMGpnU6QDMzKy7OVGYmVlTThRmZtaUE4WZmTXlRGFmZk05UZiZWVNOFFY5kn4saX6n4zDbXjhRWNtIWinpqE7HERHHRMSiTscBIOmXkv6uDe/TI+l8SY9KelDSB0bo//7U75G0XE9u3icl3SZpk6Qzyo7dOs+JwiYUSTt0OoaabooFOAOYAzwXeC3wIUlHF3WU9HrgdOBIYDawH/CJXJcVwIeAH5UXrnUTJwrrCpLeIGmppIcl/UbSy3LzTpd0r6THJN0h6U25ee+Q9GtJX5S0ATgjtV0n6fOSNkr6naRjcsts/RXfQt99JV2b3vvnkr4q6VsNPsPhklZJ+rCkB4ELJO0h6UpJg2n9V0qamfqfCbwa+IqkxyV9JbXvL2mJpA2S7pb01nHYxCcBn4yIjRFxJ3Au8I4GfecD50XE8ojYCHwy3zciFkXEj4HHxiEu2w44UVjHSToYOB94F7AX8A1gcW64416yL9TdyH7ZfkvS9NwqXgHcB+wNnJlruxvoAz4LnCdJDUJo1vfbwI0prjOAt4/wcZ4N7En2y30B2f+xC9LrfYAnga8ARMTHgH8HTouI3og4TdIuwJL0vnsDJwJfk/SSojeT9LWUXIsey1KfPYDnALfmFr0VKFxnaq/vO03SXiN8dpugnCisG/w98I2IuCEiNqfjB08BhwJExPciYnVEbImIS4B7gENyy6+OiC9HxKaIeDK13R8R50bEZmARMB2Y1uD9C/tK2gd4OfDPEfHniLgOWDzCZ9kCfDwinoqIJyPioYi4PCKeiIjHyBLZXzdZ/g3Ayoi4IH2eW4DLgeOLOkfEP0TE7g0etb2y3vT8SG7RR4CpDWLoLehLk/42wTlRWDd4LvDB/K9hYBbZr2AknZQblnoYeCnZr/+aBwrW+WBtIiKeSJO9Bf2a9X0OsCHX1ui98gYj4k+1F5KeJekbku6X9ChwLbC7pMkNln8u8Iq6bfE2sj2VsXo8Pe+aa9uVxkNHjxf0pUl/m+CcKKwbPACcWfdr+FkR8R1JzyUbTz8N2CsidgduB/LDSGWVQF4D7CnpWbm2WSMsUx/LB4EXAq+IiF2B16R2Nej/APCrum3RGxHvKXozSV9PxzeKHssB0nGGNcABuUUPAJY3+AzLC/qujYiHGn9sm8icKKzddpS0U+6xA1kieLekVyizi6T/KmkqsAvZl+kggKSTyfYoShcR9wMDZAfIp0h6JfDfRrmaqWTHJR6WtCfw8br5a8nOKqq5EniBpLdL2jE9Xi7pRQ1ifHdKJEWP/DGIi4B/SgfX9ycb7ruwQcwXAadIenE6vvFP+b4ppp3Ivj92SP+OjfaQbAJworB2u4rsi7P2OCMiBsi+uL4CbCQ7/fIdABFxB3AW8P/JvlT/Avh1G+N9G/BK4CHgU8AlZMdPWvX/gJ2B9cD1wE/q5n8JOD6dEXV2Oo7xOuAEYDXZsNhngB6emY+TnRRwP/Ar4HMR8RMASfukPZB9AFL7Z4FrUv/7GZ7gziX7tzsR+FiaHukgv23H5BsXmbVO0iXAXRFRv2dgNmF5j8KsiTTs8zxJk5RdoDYP+EGn4zJrp266ctSsGz0b+D7ZdRSrgPdExH90NiSz9vLQk5mZNeWhJzMza2q7Hnrq6+uL2bNndzoMM7Ptys0337w+Ivpb7b9dJ4rZs2czMDDQ6TDMzLYrku4fTX8PPZmZWVNOFGZm1pQThZmZNeVEYWZmTTlRmJlZU04UZmbWlBOFmZk1VclEcdeDj/K5n97Fw0/8udOhmJl1vUomipXrn+Cr19zLqo1PjtzZzKziKpko+qdOAWDw8dHcf8bMrJoqmSj6erObha1/zInCzGwklU4U3qMwMxtZJRPFLj078Kwpk1n/mA9mm5mNpJKJArK9ivXeozAzG1GFE8UUJwozsxZUNlH0T+1h0AezzcxGVFqikLSTpBsl3SppuaRPpPYLJf1O0tL0ODC1S9LZklZIWibp4LJiAw89mZm1qsw73D0FHBERj0vaEbhO0o/TvP8dEZfV9T8GmJMerwDOSc+l6OvtYeMTT/P05i3sOLmyO1ZmZiMq7RsyMo+nlzumRzRZZB5wUVruemB3SdPLiq9/anaK7IY/+swnM7NmSv0pLWmypKXAOmBJRNyQZp2Zhpe+KKkntc0AHsgtviq11a9zgaQBSQODg4Njjm3rtRQ+TmFm1lSpiSIiNkfEgcBM4BBJLwU+AuwPvBzYE/hw6q6iVRSsc2FEzI2Iuf39/WOOzWU8zMxa05bB+Yh4GPglcHRErEnDS08BFwCHpG6rgFm5xWYCq8uKyWU8zMxaU+ZZT/2Sdk/TOwNHAXfVjjtIEnAccHtaZDFwUjr76VDgkYhYU1Z8WxPF4z5GYWbWTJlnPU0HFkmaTJaQLo2IKyX9QlI/2VDTUuDdqf9VwLHACuAJ4OQSY9taxsPHKMzMmistUUTEMuCggvYjGvQP4NSy4iniaynMzEZW6QsIXMbDzGxklU4ULuNhZjaySicKDz2ZmY2s8omiVsbDzMyKVTtRuIyHmdmIKp0o+l3Gw8xsRNVOFC7jYWY2okonCpfxMDMbmRMFLuNhZtZMpROFy3iYmY2s0okCfC2FmdlInChcxsPMrKnKJ4r+qd6jMDNrpvKJoq/X9Z7MzJpxonAZDzOzppwoXMbDzKypyicKl/EwM2vOicJlPMzMmiotUUjaSdKNkm6VtFzSJ1L7vpJukHSPpEskTUntPen1ijR/dlmx5bmMh5lZc2XuUTwFHBERBwAHAkdLOhT4DPDFiJgDbAROSf1PATZGxPOBL6Z+pXMZDzOz5kpLFJF5PL3cMT0COAK4LLUvAo5L0/PSa9L8IyWprPhqXMbDzKy5Uo9RSJosaSmwDlgC3As8HBGbUpdVwIw0PQN4ACDNfwTYq2CdCyQNSBoYHBwclzhdxsPMrLFSE0VEbI6IA4GZwCHAi4q6peeivYfYpiFiYUTMjYi5/f394xKny3iYmTXWlrOeIuJh4JfAocDuknZIs2YCq9P0KmAWQJq/G7ChHfF5j8LMrLEyz3rql7R7mt4ZOAq4E7gGOD51mw/8ME0vTq9J838REdvsUZShf6rLeJiZNbLDyF3GbDqwSNJksoR0aURcKekO4LuSPgX8B3Be6n8e8K+SVpDtSZxQYmzD5Mt47Di58peWmJkNU1qiiIhlwEEF7feRHa+ob/8T8Jay4mkmX8Zj2q47dSIEM7Ou5Z/PuIyHmVkzThS4jIeZWTNOFLiMh5lZM04UuIyHmVkzThQMlfHwtRRmZttyokh8S1Qzs2JOFInLeJiZFXOiSFzGw8ysmBNF0j+1xwezzcwKOFEkfb09bPjjn3l685ZOh2Jm1lWcKJJ8GQ8zMxviRJH096ars33mk5nZME4USX/ao3AZDzOz4ZwoEpfxMDMr5kSRuIyHmVkxJ4rEZTzMzIo5UeS4jIeZ2bbKvGf2LEnXSLpT0nJJ703tZ0j6g6Sl6XFsbpmPSFoh6W5Jry8rtkZcxsPMbFtl3jN7E/DBiLhF0lTgZklL0rwvRsTn850lvZjsPtkvAZ4D/FzSCyJic4kxDtPX28PKh/7YrrczM9sulLZHERFrIuKWNP0YcCcwo8ki84DvRsRTEfE7YAUF99Yuk8t4mJltqy3HKCTNBg4CbkhNp0laJul8SXukthnAA7nFVlGQWCQtkDQgaWBwcHBc43QZDzOzbZWeKCT1ApcD74uIR4FzgOcBBwJrgLNqXQsWj20aIhZGxNyImNvf3z+usbqMh5nZtkpNFJJ2JEsSF0fE9wEiYm1EbI6ILcC5DA0vrQJm5RafCawuM756LuNhZratMs96EnAecGdEfCHXPj3X7U3A7Wl6MXCCpB5J+wJzgBvLiq9IrYyHz3wyMxtS5llPhwFvB26TtDS1fRQ4UdKBZMNKK4F3AUTEckmXAneQnTF1ajvPeIKhq7O9R2FmNqS0RBER11F83OGqJsucCZxZVkwjcRkPM7Nt+crsnF16dmDnHV3Gw8wsz4miTv9Ul/EwM8tzoqjjMh5mZsM5UdTp6+1xojAzy3GiqNPnMh5mZsM4UdTpdxkPM7NhnCjquIyHmdlwThR1XMbDzGw4J4o6LuNhZjacE0Udl/EwMxvOiaKOy3iYmQ3nRFHHZTzMzIZzoiiQ3RLVicLMDJwoCvX1TvExCjOzxImigMt4mJkNcaIo4DIeZmZDnCgK9Pf2sPEJl/EwMwMnikJ9U3uIcBkPMzMoMVFImiXpGkl3Slou6b2pfU9JSyTdk573SO2SdLakFZKWSTq4rNhG4jIeZmZDytyj2AR8MCJeBBwKnCrpxcDpwNURMQe4Or0GOAaYkx4LgHNKjK2poYvunCjMzEpLFBGxJiJuSdOPAXcCM4B5wKLUbRFwXJqeB1wUmeuB3SVNLyu+Zmr1nrxHYWbWYqKQ9JZW2posPxs4CLgBmBYRayBLJsDeqdsM4IHcYqtSW/26FkgakDQwODjYagij4jIeZmZDWt2j+EiLbduQ1AtcDrwvIh5t1rWgLbZpiFgYEXMjYm5/f38rIYyay3iYmQ3ZodlMSccAxwIzJJ2dm7Ur2TGIpiTtSJYkLo6I76fmtZKmR8SaNLS0LrWvAmblFp8JrG7tY4w/l/EwM8uMtEexGhgA/gTcnHssBl7fbEFJAs4D7oyIL+RmLQbmp+n5wA9z7Sels58OBR6pDVF1gst4mJllmu5RRMStwK2Svh0RTwOk01lnRcTGEdZ9GPB24DZJS1PbR4FPA5dKOgX4PVA71nEV2d7LCuAJ4OQxfJ5x09fbw8qH/tjJEMzMukLTRJGzRNIbU/+lwKCkX0XEBxotEBHXUXzcAeDIgv4BnNpiPKXrm9rDwP0j5UIzs4mv1YPZu6UD0W8GLoiIvwSOKi+sznMZDzOzTKuJYod04PmtwJUlxtM1XMbDzCzTaqL4F+CnwL0RcZOk/YB7ygur81zGw8ws09Ixioj4HvC93Ov7gP9eVlDdwGU8zMwyrV6ZPVPSFZLWSVor6XJJM8sOrpNqZTx8dbaZVV2rQ08XkF3n8Byyshr/ltomrNoehYeezKzqWk0U/RFxQURsSo8LgXLqZ3QJl/EwM8u0mijWS/pbSZPT42+Bh8oMrBv0TZ3iRGFmlddqongn2amxDwJrgOPp8JXT7dDf63pPZmatJopPAvMjoj8i9iZLHGeUFlWX6Ovt8TEKM6u8VhPFy/K1nSJiA9n9JSa0vqk9PuvJzCqv1UQxqXZva8jue03rdaK2Wy7jYWbW+pf9WcBvJF1GdjOhtwJnlhZVl8iX8Zi2606dDsfMrCNavTL7IkkDwBFkFWHfHBF3lBpZF8iX8XCiMLOqann4KCWGCZ8c8lzGw8ys9WMUleQyHmZmThRNuYyHmZkTRVMu42FmVmKikHR+qjZ7e67tDEl/kLQ0PY7NzfuIpBWS7pb0+rLiGi2X8TCzqitzj+JC4OiC9i9GxIHpcRWApBcDJwAvSct8TdLkEmNrmct4mFnVlZYoIuJaYEOL3ecB342IpyLid8AK4JCyYhsNl/Ews6rrxDGK0yQtS0NTtau9ZwAP5PqsSm3bkLRA0oCkgcHBwbJjdRkPM6u8dieKc4DnAQeSVaE9K7WroG8UrSAiFkbE3IiY299f/i0x+lIZj00u42FmFdXWRBERayNic0RsAc5laHhpFTAr13UmsLqdsTXSnyvjYWZWRW1NFJKm516+CaidEbUYOEFSj6R9gTnAje2MrZFaGY91Pk5hZhVVWgVYSd8BDgf6JK0CPg4cLulAsmGllcC7ACJiuaRLyUqEbAJOjYjNZcU2Gi7jYWZVV1qiiIgTC5rPa9L/TLqwIu1QovDQk5lVk6/MHsFQvSfvUZhZNTlRjKBWxsPXUphZVTlRtMBlPMysypwoWuAyHmZWZU4ULXAZDzOrMieKFriMh5lVmRNFC1zGw8yqzImiBS7jYWZV5kTRApfxMLMqc6Jogct4mFmVOVG0wGU8zKzKnCha4DIeZlZlThQtcBkPM6syJ4oWuYyHmVWVE0WL+lzGw8wqyomiRf29Pax/zAezzax6nCha1De1h0HvUZhZBZWWKCSdL2mdpNtzbXtKWiLpnvS8R2qXpLMlrZC0TNLBZcU1Vi7jYWZVVeYexYXA0XVtpwNXR8Qc4Or0GuAYYE56LADOKTGuMXEZDzOrqtISRURcC2yoa54HLErTi4Djcu0XReZ6YHdJ08uKbSxcxsPMqqrdxyimRcQagPS8d2qfATyQ67cqtXUNl/Ews6rqloPZKmiLwo7SAkkDkgYGBwdLDmuIy3iYWVW1O1GsrQ0pped1qX0VMCvXbyawumgFEbEwIuZGxNz+/v5Sg81zGQ8zq6p2J4rFwPw0PR/4Ya79pHT206HAI7Uhqm7hMh5mVlU7lLViSd8BDgf6JK0CPg58GrhU0inA74G3pO5XAccCK4AngJPLiuuZcBkPM6ui0hJFRJzYYNaRBX0DOLWsWMaLy3iYWRV1y8Hs7YLLeJhZFTlRjILLeJhZFTlRjILLeJhZFTlRjEJ/7xSX8TCzynGiGIXatRQefjKzKnGiGIXa1dm+lsLMqsSJYhRcxsPMqsiJYhRcxsPMqsiJYhRqZTzWe+jJzCrEiWKU+qZO8cFsM6sUJ4pRchkPM6saJ4pRchkPM6saJ4pRchkPM6saJ4pRchkPM6saJ4pRchkPM6saJ4pRchkPM6saJ4pRchkPM6saJ4pRchkPM6ua0m6F2oyklcBjwGZgU0TMlbQncAkwG1gJvDUiNnYivmb6XMbDzCqmk3sUr42IAyNibnp9OnB1RMwBrk6vu84uUya7jIeZVUo3DT3NAxal6UXAcR2MpSFJLuNhZpXSqUQRwM8k3SxpQWqbFhFrANLz3kULSlogaUDSwODgYJvCHc5lPMysSjpyjAI4LCJWS9obWCLprlYXjIiFwEKAuXPnRlkBNtPX28PvH3qiE29tZtZ2HdmjiIjV6XkdcAVwCLBW0nSA9LyuE7G1on+q9yjMrDranigk7SJpam0aeB1wO7AYmJ+6zQd+2O7YWtXX28MGl/Ews4roxNDTNOAKSbX3/3ZE/ETSTcClkk4Bfg+8pQOxtSRfxmPvXXfqdDhmZqVqe6KIiPuAAwraHwKObHc8Y5Ev4+FEYWYTXTedHrvd8NXZZlYlThRj4HpPZlYlThRj4DIeZlYlThRj4DIeZlYlThRj4DIeZlYlThRj5DIeZlYVThRj1Nfbw/rHfNaTmU18ThRj5DIeZlYVThRj5DIeZlYVThRjlC/jYWY2kTlRjNHWi+48/GRmE5wTxRj1T3UZDzOrBieKMXIZDzOrCieKMXIZDzOrCieKMXIZDzOrCieKMaqV8fAehZlNdE4Uz0Bfb4/PejKzCc+J4hlwGQ8zq4KuSxSSjpZ0t6QVkk7vdDzNuIyHmVVB2++Z3YykycBXgf8CrAJukrQ4Iu7obGTFamU8vnz1PUyaJCZJTBJMniSUpidJaR5b52fPYtKk3HSaJ5GWFQImTap7vbWfkIpfC7ZOk5uWQAy9D+SWZahPbRnl1iXS8vlptu3D1j5D88jPh2HvkUUxtD6Klqt1NLOO6KpEARwCrIiI+wAkfReYB3RlonjZjN2YJHHWkt92OpTKGEouQwlEDE86tUZts8xQIqwtB8MTkbaZGDY59J4jzN+2fdinaNDebJncZxthufo4Ginq0kpO1jbvNtb1tNCnlc/Rwnpa61T6KrL1jNMPnxNePou/e/V+47KukXRbopgBPJB7vQp4Rb6DpAXAAoB99tmnfZEVOOrF07jnU8ewJYLNEUSQTW8JtgREZM+bt8TQdARbtuT6RjZv8xYIgi3puTY/v56IIIAtW+pep/lb0roiyB7U2gBqy6T3ScuT61N7z9R9axzB8OVIfSIXw1C/bJ21/uTm5duj1pCWqb2urSubjmHL1mYU9am11783w9Y79Kb1ywxv27Yfw/rl5hcs32wdjfrXz62f12jdhX23DbmgT9EHK1hw9F2GbZ9ntp4W+rS0nlZ6PfP3ae+Khi76bYduSxRFqXbYpo2IhcBCgLlz547jZh+bSZPEJNR1G9LMbLx028HsVcCs3OuZwOoOxWJmZnRforgJmCNpX0lTgBOAxR2Oycys0rpqxCQiNkk6DfgpMBk4PyKWdzgsM7NK66pEARARVwFXdToOMzPLdNvQk5mZdRknCjMza8qJwszMmnKiMDOzpjQeVy12iqRB4P4xLt4HrB/HcNrBMbfH9hbz9hYvOOZ2aRTzcyOiv9WVbNeJ4pmQNBARczsdx2g45vbY3mLe3uIFx9wu4xWzh57MzKwpJwozM2uqyoliYacDGAPH3B7bW8zbW7zgmNtlXGKu7DEKMzNrTZX3KMzMrAVOFGZm1tSETxSSjpZ0t6QVkk4vmN8j6ZI0/wZJs9sf5bB4Zkm6RtKdkpZLem9Bn8MlPSJpaXr8cydirYtppaTbUjwDBfMl6ey0nZdJOrgTcebieWFu+y2V9Kik99X16fh2lnS+pHWSbs+17SlpiaR70vMeDZadn/rcI2l+B+P9nKS70r/7FZJ2b7Bs07+hNsd8hqQ/5P7tj22wbNPvlzbHfEku3pWSljZYdvTbObbePnPiPchKld8L7AdMAW4FXlzX5x+Ar6fpE4BLOhzzdODgND0V+G1BzIcDV3Z6+9bFtBLoazL/WODHZHcxPBS4odMx1/2dPEh2EVJXbWfgNcDBwO25ts8Cp6fp04HPFCy3J3Bfet4jTe/RoXhfB+yQpj9TFG8rf0NtjvkM4H+18HfT9PulnTHXzT8L+Ofx2s4TfY/iEGBFRNwXEX8GvgvMq+szD1iUpi8DjtR43f18DCJiTUTckqYfA+4ku5f49m4ecFFkrgd2lzS900ElRwL3RsRYr/IvTURcC2yoa87/zS4CjitY9PXAkojYEBEbgSXA0aUFmhTFGxE/i4hN6eX1ZHeu7BoNtnErWvl+KUWzmNP311uB74zX+030RDEDeCD3ehXbfulu7ZP+mB8B9mpLdCNIw2AHATcUzH6lpFsl/VjSS9oaWLEAfibpZkkLCua38m/RKSfQ+D9Vt21ngGkRsQayHxbA3gV9unV7v5Nsz7LISH9D7XZaGi47v8HwXrdu41cDayPingbzR72dJ3qiKNozqD8fuJU+bSepF7gceF9EPFo3+xayYZIDgC8DP2h3fAUOi4iDgWOAUyW9pm5+t27nKcAbge8VzO7G7dyqrtvekj4GbAIubtBlpL+hdjoHeB5wILCGbCinXtdt4+REmu9NjHo7T/REsQqYlXs9E1jdqI+kHYDdGNtu6LiRtCNZkrg4Ir5fPz8iHo2Ix9P0VcCOkvraHGZ9TKvT8zrgCrLd8rxW/i064RjglohYWz+jG7dzsrY2bJee1xX06artnQ6mvwF4W6SB8not/A21TUSsjYjNEbEFOLdBLF21jWHrd9ibgUsa9RnLdp7oieImYI6kfdMvxxOAxXV9FgO1M0KOB37R6A+5HdL44nnAnRHxhQZ9nl07jiLpELJ/x4faF+U28ewiaWptmuzg5e113RYDJ6Wznw4FHqkNn3RYw19f3badc/J/s/OBHxb0+SnwOkl7pGGT16W2tpN0NPBh4I0R8USDPq38DbVN3fGzNzWIpZXvl3Y7CrgrIlYVzRzzdm7HEfpOPsjOtvkt2dkJH0tt/0L2RwuwE9mwwwrgRmC/Dsf7KrLd12XA0vQ4Fng38O7U5zRgOdlZFtcDf9XhmPdLsdya4qpt53zMAr6a/h1uA+Z2wd/Gs8i++HfLtXXVdiZLYmuAp8l+wZ5CdgztauCe9Lxn6jsX+GZu2Xemv+sVwMkdjHcF2Vh+7e+5dpbhc4Crmv0NdTDmf01/p8vIvvyn18ecXm/z/dKpmFP7hbW/31zfZ7ydXcLDzMyamuhDT2Zm9gw5UZiZWVNOFGZm1pQThZmZNeVEYWZmTTlRWOkk/SY9z5b0P8Z53R8teq+ySDqurCqykh4vab2HS7ryGa7jQknHN5l/mqSTn8l7WPdyorDSRcRfpcnZwKgShaTJI3QZlihy71WWDwFfe6YraeFzlS5dxTtezgf+cRzXZ13EicJKl/ul/Gng1akO/vslTU73KrgpFV97V+p/uLJ7cnyb7KInJP0gFTFbXitkJunTwM5pfRfn3ytdAf45Sben2vt/k1v3LyVdpuweCRfnrr7+tKQ7UiyfL/gcLwCeioj16fWFkr4u6d8l/VbSG1J7y5+r4D3OTEUIr5c0Lfc+x+f6PJ5bX6PPcnRqu46spENt2TMkLZT0M+CiJrFK0lfS9vgRucKDRdspsiuuV6Yr2G2CGc9fFGYjOZ2sxn/tC3UBWSmPl0vqAX6dvsAgqz/z0oj4XXr9zojYIGln4CZJl0fE6ZJOi4gDC97rzWQF3Q4A+tIy16Z5BwEvIavL82vgMEl3kJVq2D8iQsU31zmMrFBg3mzgr8kKyF0j6fnASaP4XHm7ANdHxMckfRb4e+BTBf3yij7LAFl9oiPIroqur/vzl8CrIuLJJv8GBwEvBP4CmAbcAZwvac8m22mArHLpjSPEbNsZ71FYJ72OrP7TUrJS6nsBc9K8G+u+TP9RUq2Uxqxcv0ZeBXwnssJua4FfAS/PrXtVZAXflpJ92T8K/An4pqQ3A0U1iaYDg3Vtl0bElshKOt8H7D/Kz5X3Z6B2LOHmFNdIij7L/sDvIuKeyEovfKtumcUR8WSabhTraxjafquBX6T+zbbTOrJyETbBeI/COknA/4yIYcXqJB0O/LHu9VHAKyPiCcGkgnkAAAHMSURBVEm/JKvRNdK6G3kqN72Z7O5rm9KwyZFkxd1OI/tFnvckWXXhvPoaOEGLn6vA0zFUU2czQ/8/N5F+1KWhpSnNPkuDuPLyMTSK9diidYywnXYi20Y2wXiPwtrpMbLbu9b8FHiPsrLqSHqBsoqW9XYDNqYksT/ZrVRrnq4tX+da4G/SGHw/2S/khkMiyu7/sVtk5cTfRzZsVe9O4Pl1bW+RNEnS88gKrt09is/VqpVkw0WQ3UGt6PPm3QXsm2KCrEJuI41ivRY4IW2/6cBr0/xm2+kFdLDiq5XHexTWTsuATWkI6ULgS2RDJbekX8qDFN/W8yfAuyUtI/sivj43byGwTNItEfG2XPsVwCvJqmQG8KGIeDAlmiJTgR9K2onsV/b7C/pcC5wlSblf/neTDWtNI6va+SdJ32zxc7Xq3BTbjWTVYpvtlZBiWAD8SNJ64DrgpQ26N4r1CrI9hdvIqqP+KvVvtp0OAz4x6k9nXc/VY81GQdKXgH+LiJ9LuhC4MiIu63BYHSfpIOADEfH2Tsdi489DT2aj83/J7mNhw/UB/6fTQVg5vEdhZmZNeY/CzMyacqIwM7OmnCjMzKwpJwozM2vKicLMzJr6Txfg+gE5W8xOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
