{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, you need to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Problem Statement ##\n",
    "\n",
    "You are given a dataset containing:\n",
    "    - a training set for a linear function\n",
    "    - a test set for testing the learned hypothesis function\n",
    "    \n",
    "You will build a simple linear regression algorithm that can correctly identify the parameters of w0 and w1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for W and initializes w_0 to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the W vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    W -- initialized vector of shape (dim, 1)\n",
    "    w_0 -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    # Hint: you can use np.zeros to initialize W\n",
    "    W = np.zeros([dim,1], dtype=float)\n",
    "    w_0 = 0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(W.shape == (dim, 1))\n",
    "    assert(isinstance(w_0, float) or isinstance(w_0, int))\n",
    "    return W, w_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Gradient Descent ##\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $h_{W}(X) = W^T * X + w_{0}\\tag{1}$\n",
    "- You calculate the cost function:  $$E(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2\\tag{2}$$. \n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial w_{j}} = \\frac{1}{m} \\sum_{i=1}^m (( h_{W}(x^{(i)}) -y^{(i)}) * x_{j}^{(i)})\\tag{3}$$\n",
    "$$ \\frac{\\partial E}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (h_{W}(x^{(i)}) -y^{(i)})\\tag{4}$$\n",
    "\n",
    "The weights will be updated:\n",
    "$$ w_{j} = w_{j} - {\\alpha} * \\frac{\\partial E}{\\partial w_{j}}\\tag{5}$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Algorithm\n",
    "\n",
    "def gradient_descent(W, w_0, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes W by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    W -- the weight vector\n",
    "    w_0 -- bias, a scalar\n",
    "    X -- data of the single feature\n",
    "    Y -- true \"label\" vector \n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You need to finish the following steps:\n",
    "        1) Calculate the cost and the gradient for the current parameters. \n",
    "        2) Update the parameters using gradient descent rule for w_0 and w_1.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Get the number of training examples\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        # Calculate the heuristic function: h(x) = W.T * X + w_0\n",
    "        Y_hat = np.dot(W.T,X)+ w_0\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "        ### START CODE HERE ### \n",
    "        # Calculate cost, dw, and dw_0\n",
    "       \n",
    "        cost = np.sum(np.square(np.subtract(Y_hat,Y)))\n",
    "        dw = np.dot(X, np.subtract(Y_hat,Y).T)\n",
    "        dw_0 = np.sum(np.subtract(Y_hat,Y).T)\n",
    "        \n",
    "        cost /= 2 * m\n",
    "        dw = np.divide(dw, m)\n",
    "        dw_0 = np.divide(dw_0, m)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        # Update W and w_0\n",
    "        \n",
    "        W = np.subtract(W, np.multiply(learning_rate, dw))\n",
    "        w_0 = np.subtract(w_0, np.multiply(learning_rate, dw_0))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "        if((i % 100) == 0):\n",
    "            costs.append(cost)\n",
    "            \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        \n",
    "        \n",
    "    params = {\n",
    "        \"W\": W,\n",
    "        \"w_0\": w_0\n",
    "    }\n",
    "\n",
    "    grads = {\n",
    "        \"dw\": dw,\n",
    "        \"dw_0\": dw_0\n",
    "    }\n",
    "        \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions ###\n",
    "The predicted output is calculated as $h_{W}(X) = W^T * X + w_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(W, w_0, X):\n",
    "    '''\n",
    "    Predict the real values using learned parameters (W, w_0)\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size \n",
    "    w_0 -- bias, a scalar\n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    W = W.reshape(X.shape[0], 1)\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    Y_prediction = np.dot(W.T,X)+ w_0\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling ###\n",
    "Here you normalize features using:\n",
    "$ \\frac{x_{i} - mean}{\\sigma}$, where $\\sigma$ is the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mtx):\n",
    "    '''\n",
    "    mtx: the matrix that needs to be normalized. Note that each column represents a training example. \n",
    "         The number of columns is the the number of training examples\n",
    "    '''\n",
    "    # Calculate mean for each feature\n",
    "    # Pay attention to the value of axis = ?\n",
    "    ### START CODE HERE ###\n",
    "    mean = np.mean(mtx)\n",
    "    std = np.std(mtx)\n",
    "    mtx = (mtx-mean) /std\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.1, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the linear regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array \n",
    "    Y_train -- training labels represented by a numpy array (vector) \n",
    "    X_test -- test set represented by a numpy array\n",
    "    Y_test -- test labels represented by a numpy array (vector)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    dim = X_train.shape[0]\n",
    "    W, w_0 = initialize_with_zeros(dim)\n",
    "    \n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    \n",
    "    \n",
    "    # Gradient descent \n",
    "    ### START CODE HERE ###\n",
    "    parameters, grads, costs = gradient_descent( W, w_0, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve parameters w and w_0 from dictionary \"parameters\"\n",
    "    W = parameters[\"W\"]\n",
    "    w_0 = parameters[\"w_0\"]\n",
    "    \n",
    "    # Predict test/train set examples (â‰ˆ 2 lines of code)\n",
    "    ### START CODE HERE ###\n",
    "    Y_prediction_train = predict(W, w_0, X_train)\n",
    "    Y_prediction_test =  predict(W, w_0, X_test)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    print(\"w is \", W)\n",
    "    print(\"w_0 is \", w_0)\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"W\" : W, \n",
    "         \"w_0\" : w_0,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Start the Learning Process ###\n",
    "You can change num_iterations and learning_rate to see the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 65591548106.457443\n",
      "Cost after iteration 100: 2057986252.733239\n",
      "Cost after iteration 200: 2057830946.728015\n",
      "Cost after iteration 300: 2057830613.828608\n",
      "Cost after iteration 400: 2057830564.194410\n",
      "Cost after iteration 500: 2057830515.078139\n",
      "Cost after iteration 600: 2057830465.962981\n",
      "Cost after iteration 700: 2057830416.847990\n",
      "Cost after iteration 800: 2057830367.733164\n",
      "Cost after iteration 900: 2057830318.618505\n",
      "Cost after iteration 1000: 2057830269.504011\n",
      "Cost after iteration 1100: 2057830220.389683\n",
      "Cost after iteration 1200: 2057830171.275522\n",
      "Cost after iteration 1300: 2057830122.161525\n",
      "Cost after iteration 1400: 2057830073.047695\n",
      "Cost after iteration 1500: 2057830023.934031\n",
      "Cost after iteration 1600: 2057829974.820532\n",
      "Cost after iteration 1700: 2057829925.707198\n",
      "train accuracy: -5199896.930263967 %\n",
      "test accuracy: -9217294.72570651 %\n",
      "w is  [[ 153821.6993134 ]\n",
      " [-102359.15320045]]\n",
      "w_0 is  116573.20323822036\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('prj2house.csv', header=None)\n",
    "X_train = df[[0, 1]].values.T\n",
    "Y_train = df[2].values.reshape(-1, 1).T\n",
    "\n",
    "\n",
    "df_test = pd.read_csv('prj2house_test.csv', header=None)\n",
    "X_test = df_test[[0, 1]].values.T\n",
    "Y_test = df_test[2].values.reshape(-1, 1).T\n",
    "\n",
    "\n",
    "d = model(X_train, Y_train, X_test, Y_test, num_iterations = 1800, learning_rate = 0.1, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the learning curve ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
